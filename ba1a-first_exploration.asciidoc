== First Exploration (ch. A) ==
[[first_exploration]]

Hadoop is a remarkably powerful tool for processing data, giving us at long last mastery over massive-scale distributed computing. More than likely, that's how you came to be reading this sentence.

What you might not yet know is that Hadoop's power comes from _embracing_, not conquering, the constraints of distributed computing; and in doing so, exposes a core simplicity that makes programming it exceptionally fun.

Hadoop's bargain is thus: you must agree to write all your programs according to single certain form, which we'll call the "Map / Reduce Haiku":

    Data flutters by
    Elephants make sturdy piles
    Number becomes thought

For any such program, Hadoop's diligent elephants will intelligently schedule the tasks across ones or dozens or thousands of machines; attend to logging, retry and error handling; distribute your data to the workers that process it; handle memory allocation, partitioning and network routing; and a myriad other details that would otherwise stand between you and insight.

Let's walk through an example. For the moment, I'll omit code samples and simplify slightly so we can concentrate on the step-by-step transformation of the data. (You'll see these details later in the book.)

=== Where is Barbecue?

The Wikipedia community has geolocated a great number of its articles. Not just traditional places like Austin, TX, but a million deep of landmarks like Texas Memorial Stadium (where the Texas Longhorns football team plays) and Snow's BBQ (proclaimed "The Best Texas BBQ in the World"). 

This is the kind of data -- huge, unruly, highly-dimensional, deeply connected -- that Hadooop eats for lunch <remark>TODO cliche</remark>
gives us a long wished-for ability to make aspects of the human condition legible to computers.

For example, it's reasonable to say that examining the words

We've made the robots smarter, which is fine, but already a human could easily find out what's unique about any given place by clicking through to each page its article links to. In any natural database structure, everything you need to know is one or a few hops away. 

By contrast, let's invert the question. For every word in the English language, which have a strong geographic flavor, and if so, what is that flavor?

You can probably think of a few such words: terms like "beach" (the coasts), "barbeque" (Texas and Southeastern US) or "wine" (France, Napa Valley) will show strong place affinity; terms like "hat" or "couch" will not.
But it's hard to go beyond that, and there's certainly no way

Insight comes from data in context: places in the context of associated topics, or topics in the context of associated locations. When your data is far too large to fit on a single machine,

This gives a sense of what we mean by 'locality', the most important concept in the bookfootnote:[Please discard any geographic context of the word "local": for the rest of the book it will always mean "held in the same computer location"]. In the first case, putting places in the context of their associated pages requires only simple mouse clicks or database calls. In the second case, putting every word in context of all associated locations requires that you dismantle and reassemble the whole dataset in stages.

Hadoop simultaneously unlocks the ability to extract meaning from data that is
massive, deeply connected, imperfect and non-local

It's this kind of challenge -- data that is massive, highly-dimensional, imperfect, 

Here's how Hadoop makes answering a deep question like that simple.

=== Preparation


I hope that question is interesting enough to stand on its own. But it also touches on the type of practical explorations we'll perform in the rest of the book.

* _Geographic Analysis_ to find spatial relationships among objects: here, words from a corpus, but it could be signs of an epidemic from disease reports, or common browsing behavior among visitors to your website.
* _Linguistic Analysis_, to extract meaning from text: here, attaching geographic locations to terms, but it could be authorship analysis for legal discovery, or the baseline for real-time "trending topic" detection.
* _Statistical Analysis_, to simplify and summarize patterns in data: even simple counts and frequencies require some craft at large scale; measures that require any global context, like the median, become fiendish.
* _Raw data munging_ -- We'll hide the details of this for now, but too often preparing the data takes longer than analyzing it.

We'll explore each of these topics, along with anaysis of time series, graphs and server logs. 

We will jointly discover two things
taking as a whole the terms that have a strong geographic flavor, we should largely see cultural terms (foods, sports, etc)

Terms like "beach" or "mountain" will clearly 

Common words like "couch" or "hair" 

Words like 'town' or 'street' will be 

You don't have to stop exploring when you find a new mystery, but no data exploration is complete until you uncover at least one.

Next, we'll choose some _exemplars_: familiar records to trace through
 "Barbeque" should cover ;

==== Data and features

=== Summarize every page on Wikipedia

[[baldridge_bbq_wine]]
.Not the actual output, but gives you the picture; TODO insert actual results
image::images/baldridge-bbq_wine_beach_mountain-480.jpg[Location affinity for Beach, Mountain, BBQ and Wine]

==== Summarize every page on Wikipedia

First, we will summarize each topic by preparing its "word bag" -- a simple count of the words on its wikipedia page. This snippet of the <<wp_lexington_article>> text:

[[wp_lexington_article]]
._Wikipedia article on "Lexington, Texas"_
______
Lexington is a town in Lee County, Texas, United States. ... Snow's BBQ, which Texas Monthly called "the best barbecue in Texas" and The New Yorker named "the best Texas BBQ in the world" is located in Lexington.
______

turns into <<wp_lexington_wordbag,this wordbag>>:

[[wp_lexington_wordbag]]
._Wordbag for "Lexington, Texas"_
------
Lexington,_Texas {("texas",4)("lexington",2),("best",2),("bbq",2),("barbecue",1), ...}
------

You can carry out this process on each article separately, in any order, and without reference to information from elsewhere. That's important! Among other things, it lets us parallelize the process across as many machines as we care to afford. We'll call this type of step a "transform": it's independent, non-order-dependent, and isolated.

==== Bin by Location

Next we will annotate each wordbag with the geolocation of its article. Those are kept in a different data file, but we can merge the two files by joining each article metadata record with the wordbag having the same id. Here's its output:

[[wp_lexington_wordbag_and_coords]]
._Wordbag with coordinates_
------
Lexington,_Texas -97.01 30.41 023130130 {("texas",4)("lexington",2),("best",2),("bbq",2),("barbecue",1), ...}
------

The funny-looking number in the fourth column is the label of one cell in a <<geographic grid,quadkey_central_texas>> that
footnote:[a "quadkey", described later in <<quadkey,"Geographic Data">>]


[[quadkey_central_texas]]
.Grid Tiles for Central Texas
image::images/Quadtree-google_maps_screenshot.png[Grid tiles for Central Texas]

Next, combine the individual word bags to find each grid cell's word bag:

------
023130130 {(("many", X),...,("texas",X),...,("town",X)...("longhorns",X),...("bbq",X),...}
------

==== A pause, to think

Let's pause, take a breath, and examine the fundamental pattern that got us here. We

. transformed articles into wordbags
. augmented each wordbag with coordinates, using a join
. converted each article's precise point into the coarse-grained tile it sits on
. brought all wordbags for each tile together;
. merging each tile's word counts into a single combined wordbag.

It's a simple sequence of _transforms_ (operations on each record in isolation: steps 1, 3 and 5) and _reshapes_ -- operations that combine multiple rows, from different tables (the join in step 2) or in the same dataset (the group in step 4).

In doing so, we've turned articles that have a geolocation into coarse-grained regions that have implied frequencies for words. The particular frequencies arise from this combination of forces:

* _signal_: Terms that describe aspects of the human condition specific to each region, like "longhorns" or "barbecue", and direct references to place names, such as "Austin" or "Texas"
* _background_: The natural frequency of each term -- "second" is used more often than "syzygy" -- slanted by its frequency in geo-locatable texts (the word "town" occurs far more frequently than its natural rate, simply because towns are geolocatable).
* _noise_: Deviations introduced by the fact that we have a limited sample of text to draw inferences from.

Our next task -- the sprint home -- is to separate the signal from the background and (as much as possible) from the noise.

==== Pulling signal from noise

To isolate the signal, we'll pull out a trick called <<pmi,"Pointwise Mutual Information" (PMI)>>. Though it may sound like an insurance holding company, in fact PMI is a simple approach to isolate the noise and background. It compares the following:

* the rate the term 'barbecue' is used
* the rate that terms are used on grid cell 023130130
* the rate the term 'barbecue' is used on grid cell 023130130

Just as above, we can transform and reshape to get those figures:

* group the data by term; count occurrences
* group the data by tile; count occurrences
* group the data by term and tile; count occurrences
* count total occurrences
* combine those counts into rates, and form the PMI scores.

Rather than step through each operation, I'll wave my hands and pull its output from the oven:

------
023130130 {(("texas",X),...,("longhorns",X),...("bbq",X),...,...}
------

As expected, in <<baldridge_bbq_wine>> you see BBQ loom large over Texas and the Southern US; Wine, over the Napa Valley.

footnote:[This is a simplified version of work by Jason Baldrige, Ben Wing (TODO: rest of authors), who go farther and show how to geolocate texts _based purely on their content_. An article mentioning barbecue and Willie Nelson would be placed near Austin, TX; one mentioning startups and trolleys in San Francisco. See: Baldridge et al (TODO: reference)]

==== Takeaway #1: Simplicity

We accomplished an elaborate data exploration, yet at no point did we do anything complex. Instead of writing a big hairy monolithic program, we wrote a series of simple scripts that either _transformed_ or _reshaped_ the data.

As you'll see later, the scripts are readable and short (none exceed a few dozen lines of code). They run easily against sample data on your desktop, with no Hadoop cluster in sight; and they will then run, unchanged, against the whole of Wikipedia on dozens or hundreds of machines in a Hadoop cluster.

That's the approach we'll follow through this book: develop simple, maintainable transform/reshape scripts by iterating quickly and always keeping the data visible; then confidently transition those scripts to production as the search for a question becomes the rote production of an answer.

The challenge, then, isn't to learn to "program" Hadoop -- it's to learn how to think at scale, to choose a workable series of chess moves connecting the data you have to the insight you need. In the first part of the book, after briefly becoming familiar with the basic framework, we'll proceed through a series of examples to help you identify the key locality and thus the transformation each step calls for. In the second part of that book, we'll apply this to a range of interesting problems and so build up a set of reusable tools for asking deep questions in actual practice.
