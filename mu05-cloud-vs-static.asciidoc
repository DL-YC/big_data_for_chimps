== Cloud vs Static

* your cluster is too big and too small
* re-size mid job -- why not?
* tuning cluster to the job is way easier than v/v
* your data isn't that big (yet)
* with small teams, worry about downtime not utilization
* encourage profligacy -- humans are important, robots are cheap. "Don't want data scientist
  watching a cluster whose hourly rate is smaller than hers."

=== How big is a Terabyte? ===

an m1.large:
  - 3 map tasks 300 MB raw input, 340 MB raw output (150 MB compressed), in 2 min
    - 1 GB in, 1 GB out (450 MB compressed)
  - 2 reduce tasks 700 MB in, 1.7 GB out, 50% spill
    - 1.5GB in, 3.5 GB out, 4 mins.

an m2.2xlarge:
  - 5 map tasks, each 460 MB raw input, 566 MB raw output (260 MB compressed) 1.5 min
    - 2.3 GB in, 2.8 GB out (1.3 GB compressed) -> 2 GB / m2.2xl*min

  - overall 50 GB in, 53 GB out, 12.5 min * 6 m2.2xl = $1.12
  - for 1 TB, ~ 30 m2.2xl 50 min

colobus-master-0 /usr/lib/hadoop$ hdp-du s3n://bigdata.chimpy.us/data/results/wikipedia/full
Found 5 items
s3n://bigdata.chimpy.us/data/results/wikipedia/full/page_metadata      	     2233601837	         2.1 GB
s3n://bigdata.chimpy.us/data/results/wikipedia/full/pagelinks          	    49777662270	        46.4 GB
s3n://bigdata.chimpy.us/data/results/wikipedia/full/pageviews          	   846600801080	       788.5 GB
s3n://bigdata.chimpy.us/data/results/wikipedia/full/redirects_page_metadata	      400190601	       381.7 MB
s3n://bigdata.chimpy.us/data/results/wikipedia/full/undirected_pagelinks	    13182759459	        12.3 GB
                                                       5 entries       	   912195015247	       849.5 GB

wikipedia corpus: 	40 GB
Hour-by-hour weather: 	12 GB

Every pageview of the World Cup website during the year: 	1TB
Every pitch of every MLB baseball game, with full game state, 2007-2011 < 100 GB

10 trillion digits of Pi
Google Books N-Grams			2 TB
Common Crawl Web Corpus			60 TB
Planet.osm contains the entire planet. This is a snapshot of the current data, usually from last Wednesday. This is almost 20 GB compressed or 150 GB uncompressed XML.


From http://developer.yahoo.com/blogs/hadoop/posts/2010/05/scalability_of_the_hadoop_dist/ -- May 5, 2010

	Throughput
Get block locations	126,119 ops/s
Create new block	5,600 ops/s

Throughput
Average read throughput	66 MB/s
Average write throughput	40 MB/s


terasort

 On a 100-node cluster with a quad dual-core CPU hardware the running time should be roughly within
 10 minutes (one of our customers sorted 1TB in 6 minutes on a 76-node cluster, the numbers are
 likely to go down with new 12-core CPU machines). 

March 30, 2010
 
 4 1TB hard disks in a JBOD (Just a Bunch Of Disks) configuration
2 quad core CPUs, running at least 2-2.5GHz
16-24GBs of RAM (24-32GBs if youâ€™re considering HBase)
Gigabit Ethernet

For the below: the *must* haves, not the "gee that would be swell"

How much data under management do customers have, month on month, bringing in?
* cold load?
* hot load?
How many disparate data sources?

How large is analytic team?

* "I use Amazon cloud infrastructure; it's important you run on AWS"
* "I don't care where
* Exploratory data analysis
