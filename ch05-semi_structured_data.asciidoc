== Semi-Structured Data ==

image::images/semistructured_data_workflow.png[Semistructured Data Workflow]

* Raw data, as it comes from the source. This could be unstructured (lines of text,
* Source domain models:
* Target domain models:
* Transformer:

You'll be tempted to move code too far to the right -- to put transformation code into
Resist the urge. At the beginning of the project you're thinking about the details of extracting the data, and possibly still puzzling out the shape of the target domain models.
Make the source domain models match the raw data as closely as reasonable, doing only the minimum effective work to make the data active.

Separate the model's _properties_ (fundamental intrinsic data), its _metadata_ (details of processing), and its _derived variables_ (derived values).

=== Wikipedia Datasets ===

The wikipedia datasets came from a few different places


==== Wikipedia Target Domain Model ====

First step is to give some thought to the target domain model. There's a clear match to the Schema.org `Article` type, itself a subclass of `CreativeWork`, so we'll use the property names and descriptions:

--------------------
class Wikipedia::WpArticle
  field      :id,                     String,    doc: "Unique identifier for the article; it forms the tail-end of the traditional URL"
  field      :wp_page_id,             Integer,   doc: "Numeric serial ID for the page (as opposed to the article's topic)"
  field      :name,                   String,    doc: "Topic name (human-readable)"
  field      :description,            String,    doc: "Short abstract of the content"
  field      :keywords,    Array, of: String,    doc: "List of freeform tags for the topic"
  field      :article_body,           String     doc: "Contents of the article"
  field      :coordinates, Geo::Coordinates,     doc: "Primary location for the topic"
  field      :content_location, Geo::Place,      doc: "The location of the topic"
  field      :in_language,            String,    doc: "Language identifier"
  collection :same_as_ids,            String,    doc: "Articles that redirect to this one"
  collection :wp_links,               Hyperlink, doc: "Links to Wikipedia Articles from this article"
  collection :external_links,         Hyperlink, doc: "Links to external sites from this article"
  field      :wp_project,             String,    doc: "Wikipedia project identifier; main wikipedia a, wikibooks b, wiktionary d, wikimedia m, wikipedia mobile mw, wikinews n, wikiquote q, wikisource s, wikiversity v, mediawiki w"
  field      :extended_properties,    Hash,      doc: "Interesting properties for this topic, extracted by DBpedia. For example, the topic 'Abraham Lincoln' has properties vice_president:         \"Andrew_Johnson\", spouse: \"Mary_Todd_Lincoln\" and so forth."
end
--------------------

==== WikipediaPageview ====

This shows up as a `.tsv` file with columns for `lang_and_project`, `page_id`, `request_count`, `transferred_bytesize`. Since it's a tsv, parsing is as easy as defining the model and calling `from_tuple`:

--------------------
class Wikipedia::RawPageview < Wikipedia::Base
  field :lang_and_project,     String
  field :id,                   String
  field :request_count,        Integer
  field :transferred_bytesize, Integer
end
mapper do
  input > from_tsv >
    ->(vals   ){ Wikipedia::RawPageview.from_tuple(vals) } >
    to_json > output
end
--------------------


We're going to make the following changes:

* split the `lang_and_project` attribute into `in_language` and `wp_project`. They're different properties, and there's no good reason to leave them combined.
* add the numeric id of the article
* add the numeric id of the redirect-resolved article: this will make it easy to group page views under their topic

===== Translation =====

The translation is light, and the original model is of little interest, so we can just put the translation code into the raw model.
First, we'll add the `in_language` and `wp_project` properties as virtual accessors:

--------------------
class Wikipedia::RawPageview < Wikipedia::Base
  # ... (cont) ...
  
  def in_language() lang_and_project.split('.')[0]       ; end
  def wp_project() lang_and_project.split('.')[1] || 'a' ; end

  def to_wikipedia_pageview
    Wikipedia::WikipediaPageview.receive(
      id: id, request_count: request_count, transferred_bytesize: transferred_bytesize,
      in_language: in_language, wp_project: wp_project)
  end
end

mapper do
  input > from_tsv >
    ->(vals   ){ Wikipedia::RawPageview.from_tuple(vals) } >
    ->(raw_rec){ raw_rec.to_wikipedia_pageview }           >
    to_json > output
end
--------------------

==== Wikipedia Corpus ====

The Wikipedia corpus is a good introduction to handling XML input. It lacks some of the really hairy aspects of XML handling foonote:[FIXME: make into references see the section on XML for a list of drawbacks], which will let us concentrate on the basics.

The raw data is a single XML file, 8 GB compressed and about 40 GB uncompressed. After a brief irrelevant header, it's simply several million records that look like this:

--------------------
  <page>
    <title>Abraham Lincoln</title>
    <id>307</id>
    <revision>
      <id>454480143</id>
      <timestamp>2011-10-08T01:36:34Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor />
      <comment>Dating maintenance tags: {{Page needed}}</comment>
      <text xml:space="preserve">...(contents omitted; they
are XML-encoded (that&apos;s helpful)
  with spacing      preserved 
including newlines)...</text>
    </revision>
  </page>
--------------------

I've omitted the article contents, which are cleanly XML encoded and not in a CDATA block, so there's no risk of a spurious XML tag in an inappropriate place; avoid CDATA blocks if you can. The contents preserve all the whitespace of the original body, so we'll need to ensure that our XML parser does so as well.

From this, we'd like to extract the title, numeric page id, timestamp, and text, and re-emit each record in one of our favorite formats.

Now we meet our first two XML-induced complexities: _splitting_ the file among mappers, so that you don't send the first half of an article to one task and the rest to a different one; and _recordizing_ the file from a stream of lines into discrete XML fragments, each describing one article.

===== Custom Splitter / InputFormat =====

At 40GB uncompressed, the articles file will occupy about 320 HDFS blocks (assuming 128MB blocks), each destined for its own mapper. However, the division points among blocks is arbitrary: it might occur in the middle of a word in the middle of a record with no regard for your feelings about the matter. However, if you do it the courtesy of pointing to the first point within a block that a split _should_ have occurred, Hadoop will handle the details of patching it onto the trailing end of the preceding block. Pretty cool.

You need to ensure that Hadoop splits the file at a record boundary: after `</page>`, before the next `<page>` tag.

If you're

Writing an input format and splitter is only as hard as your input format makes it, but it's the kind of pesky detail that lies right at the "do it right" vs "do it (stupid/simpl)ly" decision point. Luckily there's a third option, which is to steal somebody else's code footnote:[see Hadoop the Definitive Guide, chapter FIXME: XX for details of building your own splitter]. Oliver Grisel (@ogrisel) has written an Wikipedia XML reader as a raw Java API reader in the http://mahout.apache.org/[Mahout project], and as a Pig loader in his https://github.com/ogrisel/pignlproc[pignlproc] project.
Mahout's XmlInputFormat  (https://github.com/apache/mahout/blob/trunk/integration/src/main/java/org/apache/mahout/text/wikipedia/XmlInputFormat.java[src])

===== Brute Force =====

If all you need to do is yank the data out of it's ill-starred format, or if the data format's complexity demands the agility of a high-level language, you can use Hadoop Streaming as a brute-force solution. In this case, we'll still be reading the data as a stream of lines, and use native libraries to do the XML parsing. We only need to ensure that the splits are correct, and the  `StreamXmlRecordReader` (http://hadoop.apache.org/mapreduce/docs/r0.21.0/api/org/apache/hadoop/streaming/StreamXmlRecordReader.html[doc] / https://github.com/apache/hadoop-common/blob/branch-0.21/mapreduce/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamXmlRecordReader.java[source]); 
that ships with Hadoop is sufficient.

--------------------
class Wikipedia::RawArticle
  field :title,     Integer
  field :id,        Integer
  field :revision,  Wikipedia::RawArticleRevision
end
class Wikipedia::RawArticleRevision
  field :id,        Integer
  field :timestamp, Time
  field :text,      String
end
--------------------

