== Tuning

There are enough knobs and twiddles on a hadoop installation to fully stock the cockpit of a 747, and some of them interact surprisingly. Here's our approach to configuring and optimizing Hadoop.

Fundamentally, what you want to know is

* What are the maximum practical capabilities of my system, and are they reasonable?
* How do I tell what constraints a job is hitting, and whether it's reasonable to optimize it?
* If I need to optimize a job, what settings are relevant and what are their tradeoffs?

Coarsely speaking, jobs are constrained by one of these four capabilities:

* RAM: Available memory per node,
* Disk IO: Disk throughput,
* Network IO: Network throughput, and
* CPU: Computational throughput.

In practice,

* You either have enough RAM, or you do not -- make sure you do.
* If throughput isn't near the IO-bound limit, there is probably a discoverable reason.

Your job is to

* **Recognize when your job significantly underperforms** the practical expected throughput, and if so, whether you should worry about it. If your job's throughput on a small cluster is within a factor of two of a job that does nothing, it's not worth tuning. If that job runs nightly and costs $1000 per run, it is.
* **Identify the limiting capability**.
* **Ensure there's enough RAM**. If there isn't, you can adjust your the memory per machine, the number of machines, or your algorithm design.
* **Not get in Hadoop's way**. There are a few easily-remedied things to watch for that will significantly hamper throughput by causing unneccesary disk writes or network traffic.
* **When reasonable, adjust the RAM/IO/CPU tradeoffs**. For example, with plenty of RAM and not too much data, increasing the size of certain buffers can greatly reduce the number of disk writes: you've traded RAM for Disk IO.

=== Measuring your system: theoretical limits ===

What we need here is a ready-reckoner for calculating the real costs of processing. We'll measure two primary metrics:

* throughput, in `GB/min`.
* machine cost in `$/TB` -- equal to `(number of nodes) * (cost per node hour) / (60 * throughput)`. This figure accounts for tradeoffs such as spinning up twice as many nodes versus using nodes with twice as much RAM. To be concrete, we'll use the 2012 Amazon AWS node pricing; later in this chapter we'll show how to make a comparable estimate for physical hardware.

If your cluster has a fixed capacity, throughput has a fixed proportion to cost and to engineer time. For an on-demand cluster, you should 

_note: I may go with min/TB, to have them be directly comparable. Throughput is typically rendered as quantity/time, so min/TB will seem weird to some. However, min/TB varies directly with $/TB, and is slightly easier to use for a rough calculation in your head._

* Measure disk throughput by using the `cp` (copy) command to copy a large file from one disk to another on the same machine, compressed and uncompressed.
* Measure network throughput by using `nc` (netcat) and  `scp` (ssh copy) to copy a large file across the network, compressed and uncompressed.
* Do some increasingly expensive computations to see where CPU begins to dominate IO. 
* Get a rough understanding of how much RAM you should reserve for the operating system's caches and buffers, and other overhead -- it's more than you think.

=== Measuring your system: imaginary limits ===

* http://www.textuality.com/bonnie/advice.html[Bonnie] for disk 
* http://www.coker.com.au/bonnie/[Bonnie++]  for disk 
* http://www.phoronix-test-suite.com/?k=downloads[Phoronix] for a broad-based test

=== Measuring your system: practical limits ===

* Understand the practical maximum throughput baseline performance against the fundamental limits of the system


* If your runtime departs significantly from the practical maximum throughput

Tuning your cluster to your job makes life simple
* If you are hitting a hard constraint (typically, not enough RAM)



== Hadoop System configurations ==

**Cluster Layout**

* Choose the number of mappers and reducers
  - To make best use of your CPUs, you want the number of running tasks to be at least `cores-1`; as long as there's enough ram, go as high as mappers = `cores * 3/4` and reducers = `cores * 1/2`.  For a cluster purpose-built to run jobs with minimal reduce tasks, run as many mappers as cores.
  - The total heap allocated to the datanode, tasktracker, mappers and reducers should be less than but close to the size of RAM on the machine.
  - The mappers should get at least twice as much total ram as your typical mapper output size (which is to say, at least twice as much ram as your HDFS block size).
  - The more memory on your reducers the better. If at all possible, size your cluster to at least half as much RAM as your reduce input data size. 

* If you're going to run two master nodes, you're a bit better off running one master as (namenode only) and the other master as (jobtracker, 2NN, balancer) -- the 2NN should be distinctly less utilized than the namenode. This isn't a big deal, as I assume your master nodes never really break a sweat even during heavy usage.

**Memory**

Here's a plausible configuration for a 16-GB physical machine with 8 cores:

--------------------  
  `mapred.tasktracker.reduce.tasks.maximum`   = 2
  `mapred.tasktracker.map.tasks.maximum`      = 5
  `mapred.child.java.opts`                    = 2 GB
  `mapred.map.child.java.opts`                = blank (inherits mapred.child.java.opts)
  `mapred.reduce.child.java.opts`             = blank (inherits mapred.child.java.opts)
  
  total mappers' heap size                    = 10   GB (5 * 2GB)
  total reducers' heap size                   =  4   GB (2 * 2GB)
  datanode heap size                          =  0.5 GB
  tasktracker heap size                       =  0.5 GB
  .....                                         ...
  total                                       = 15   GB on a 16 GB machine
--------------------

  - It's rare that you need to increase the tasktracker heap at all. With both the TT and DN daemons, just monitor them under load; as long as the heap healthily exceeds their observed usage you're fine.

  - If you find that most of your time is spent in reduce, you can grant the reducers more RAM with `mapred.reduce.child.java.opts` (in which case lower the child heap size setting for the mappers to compensate).

* It's standard practice to disable swap -- you're better off OOM'ing footnote[OOM = Out of Memory error, causing the kernel to start killing processes outright] than swapping. If you do not disable swap, make sure to reduce the `swappiness` sysctl (5 is reasonable). Also consider setting `overcommit_memory` (1) and `overcommit_ratio` (100). Your sysadmin might get angry when you suggest these changes -- on a typical server, OOM errors cause pagers to go off. A misanthropically funny T-shirt, or whiskey, will help establish your bona fides.

* `io.sort.mb` default `X`, recommended at least `1.25 * typical output size` (so for a 128MB block size, 160). It's reasonable to devote up to 70% of the child heap size to this value.

* `io.sort.factor`: default `X`, recommended `io.sort.mb * 0.x5 * (seeks/s) / (thruput MB/s)`
  - you want transfer time to dominate seek time; too many input streams and the disk will spend more time switching among them than reading them.
  - you want the CPU well-fed: too few input streams and the merge sort will run the sort buffers dry.
  - My laptop does 76 seeks/s and has 56 MB/s throughput, so with `io.sort.mb = 320` I'd set `io.sort.factor` to 27.
  - A server that does 100 seeks/s with 100 MB/s throughput and a 160MB sort buffer should set `io.sort.factor` to 80.

* `io.sort.record.percent` default `X`, recommended `X` (but adjust for certain jobs)

* `mapred.reduce.parallel.copies`: default `X`, recommended  to be in the range of `sqrt(Nw*Nm)` to `Nw*Nm/2`  You should see the shuffle/copy phase of your reduce tasks speed up.

* `mapred.job.reuse.jvm.num.tasks` default `1`, recommended `-1`. If a job requires a fresh JVM for each process, you can override that in its jobconf.

* You never want Java to be doing stop-the-world garbage collection, but for large JVM heap sizes (above 4GB) they can become especially dangerous. If a full garbage collect takes too long, sockets can time out, causing loads to increase, causing garbage collects to happen, causing... trouble, as you can guess.

* Given the number of files and amount of data you're storing, I would set the NN heap size agressively - at least 4GB to start, and keep an eye on it. Having the NN run out of memory is Not Good. Always make sure the secondary name node has the same heap setting as the name node.

**Handlers and threads**

* `dfs.namenode.handler.count`: default `X`, recommended: `(0.1 to 1) * size of cluster`, depending on how many blocks your HDFS holds.
* `tasktracker.http.threads` default `X`, recommended `X`

* Set `mapred.reduce.tasks` so that all your reduce slots are utilized -- If you typically only run one job at a time on the cluster, that means set it to the number of reduce slots. (You can adjust this per-job too). Roughly speaking: keep `number of reducers * reducer memory` within a factor of two of your reduce data size.

* `dfs.datanode.handler.count`:  controls how many connections the datanodes can maintain. It's set to 3 -- you need to account for the constant presence of the flume connections. I think this may be causing the datanode problems. Something like 8-10 is appropriate.
* You've increased `dfs.datanode.max.xcievers` to 8k, which is good.

**Storage**
  
* `mapred.system.dir`: default `X` recommende `/hadoop/mapred/system` Note that this is a path on the HDFS, not the filesystem).

* Ensure the HDFS data dirs (`dfs.name.dir`, `dfs.data.dir` and `fs.checkpoint.dir`), and the mapreduce local scratch dirs (`mapred.system.dir`) include all your data volumes (and are off the root partition). The more volumes to write to the better. Include all the volumes in all of the preceding. If you have a lot of volumes, you'll need to ensure they're all attended to; have 0.5-2x the number of cores as physical volumes.

* Solid-state drives are unjustifiable from a cost perspective. Though they're radically better on seek they don't improve performance on bulk transfer, which is what limits Hadoop. Use regular disks.

* Do not construct a RAID partition for Hadoop -- it is happiest with a large JBOD. (There's no danger to having hadoop sit on top of a RAID volume; you're just hurting performance).

* We use `xfs`; I'd avoid `ext3`.

* Set the `noatime` option (turns off tracking of last-access-time) -- otherwise the OS updates the disk on every read.

* Increase the ulimits for open file handles (`nofile`) and number of processes (`nproc`) to a large number for the `hdfs` and `mapred` users: we use `32768` and `50000`.

**Other**

* `mapred.map.output.compression.codec`: default XX, recommended ``. Enable Snappy codec for intermediate task output.
  - `mapred.compress.map.output`
  - `mapred.output.compress`
  - `mapred.output.compression.type`
  - `mapred.output.compression.codec`

* `mapred.reduce.slowstart.completed.maps`
* `mapred.map.tasks.speculative.execution`: default: `true`, recommended: `true`. Speculative execution (FIXME: explain). So this setting makes jobs finish faster, but makes cluster utilization higher; the tradeoff is typically worth it, especially in a development environment. Disable this for any map-only job that writes to a database or has side effects besides its output. Also disable this if the map tasks are expensive and your cluster utilization is high.
* `mapred.reduce.tasks.speculative.execution`: default `false`, recommended: `false`.

* (hadoop log location): default `/var/log/hadoop`, recommended `/var/log/hadoop` (usually). As long as the root partition isn't under heavy load, store the logs on the root partition. Check the Jobtracker however -- it typically has a much larger log volume than the others, and low disk utilization otherwise. In other words: use the disk with the least competition.

* `fs.trash.interval` default `1440` (one day), recommended `2880` (two days). I've found that files are either a) so huge I want them gone immediately, or b) of no real concern. A setting of two days lets you to realize in the afternoon today that you made a mistake in the morning yesterday, 

* Unless you have a ton of people using the cluster, increase the amount of time the jobtracker holds log and job info; it's nice to be able to look back a couple days at least. Also increase `mapred.jobtracker.completeuserjobs.maximum` to a larger value. These are just for politeness to the folks writing jobs.
  - `mapred.userlog.retain.hours`
  - `mapred.jobtracker.retirejob.interval`
  - `mapred.jobtracker.retirejob.check`
  - `mapred.jobtracker.completeuserjobs.maximum`
  - `mapred.job.tracker.retiredjobs.cache`
  - `mapred.jobtracker.restart.recover`
