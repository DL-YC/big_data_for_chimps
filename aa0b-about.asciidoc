[[about]]
== About  ==

[[about_coverage]]
=== What this book covers ===

'Big Data for Chimps' shows you how to solve hard problems using simple, fun, elegant tools. 

It contains

* Detailed example programs applying Hadoop to interesting problems in context
* Advice and best practices for efficient software development
* How to think at scale -- equipping you with a deep understanding of how to break a problem into efficient data transformations, and of how data must flow through the cluster to effect those transformations.

All of the examples use real data, and describe patterns found in many problem domains:

* Statistical Summaries
* Identify patterns and groups in the data
* Searching, filtering and herding records in bulk
* Advanced queries against spatial or time-series data sets.

This is not a beginner's book. There is emphasis on simplicity and fun, which should make this book especially appealing to beginners, but this is not an approach you'll outgrow. We emphasize simplicity and fun because it's a far more powerful approach, and one that generates the most value: humans are important, robots are cheap. The code you see is adapted from programs we write at Infochimps. There are sections describing how and when to integrate custom components or extend the toolkit, but simple high-level transformations meet almost all of our needs.

Most of the chapters have exercises included. If you're a beginning user, I highly recommend you work out at least one exercise from each chapter. Let me put that more strongly: if you're a beginning user, you should not *read* this book -- you should have it open next to you while you *write* whatever code each chapter inspires you to produce. The book's website has sample solutions, and data sets to compare against your output.

Feel free to hop around among chapters; the application chapters don't have large dependencies on earlier chapters. 

[[about_is_for]]
=== Who this book is for ===

You should be familiar with at least one programming language, but it doesn't have to be Ruby. Ruby is a very readable language, and the code samples provided should map cleanly to languages like Python or Scala. Familiarity with SQL will help a bit, but isn't essential.

All of the code in this book will run unmodified on your laptop computer and on an industrial-strength Hadoop cluster (though you will want to use a reduced data set for the laptop).

You don't _need_ to have an existing Hadoop installation, but you'll learn best by spending some time on a real environment. Setting up a cluster is outside of this book's scope, but (TODO: ref) has instructions for getting a test cluster, and references to help you set up a real cluster

Most importantly, you should have an actual project in mind that requires a big data toolkit to solve -- a problem that requires scaling out across multiple machines. If you don't already have a project in mind but really want to learn about the big data toolkit, take a quick browse through the exercises. At least a few of them should have you jumping up and down with excitement to learn this stuff.

[[about_is_not_for]]
=== Who this book is not for ===

This is not "Hadoop the Definitive Guide" (that's been written, and well); this is more like "Hadoop: a Highly Opinionated Guide".  The only coverage of how to use the bare Hadoop API is to say "In most cases, don't". We recommend storing your data in one of several highly space-inefficient formats and in many other ways encourage you to willingly trade a small performance hit for a large increase in programmer joy. The book has a relentless emphasis on writing *scalable* code, but no content on writing *performant* code beyond the advice that the best path to a 2x speedup is to launch twice as many machines.

That is because for almost everyone, the cost of the cluster is far less than the opportunity cost of the data scientists using it. If you have not just big data but huge data -- let's say somewhere north of 100 terabytes -- then you will need to make different tradeoffs for jobs that you expect to run repeatedly in production. 

The book does have some content on machine learning with Hadoop, on provisioning and deploying Hadoop, and on a few important settings. But it does not cover advanced algorithms, operations or tuning in any real depth.

[[about_how_written]]
=== How this book is being written ===

I plan to push chapters to the publicly-viewable http://github.com/infochimps-labs/big_data_for_chimps['Hadoop for Chimps' git repo] as they are written, and to post them periodically to the http://blog.infochimps.com[the Infochimps blog] after minor cleanup.

We really mean it about the github thing -- please https://github.com/blog/622-inline-commit-notes[comment] on the text, http://github.com/infochimps-labs/big_data_for_chimps/issues[file issues] and send pull requests. 

However! We might not use your feedback, no matter how dazzlingly cogent it is. Also, we are soliciting comments from readers -- but we are not seeking out content from collaborators. Do not run off and craft a dazzlingly lucid explanation of some topic, far excelling whatever hackish tripe I would have coughed out, and then expect us to include it footnote:[Do craft dazzlingly lucid explanations, though! Always!. I have strong opinions about what the book should cover and a fixed budget of pages; and while I am trying my hand at author, I have no interest in being an editor. Don't prepare original content unless you get in touch first.


=== Organization ===

* Practice   -- A real question, on real data, as we'd really do it. I'll especially try to highlight where you should be disciplined, where you should be clever, and where you should keep it simple
* Patterns   -- Reasoning forward from the practical case to show the general patterns that guide its execution
* Principles -- Lastly, the larger philosophy that should guide your thinking

