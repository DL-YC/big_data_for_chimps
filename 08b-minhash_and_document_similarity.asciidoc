
==== Document Authorship ====


> As we mentioned earlier, all previously known constructions of JL-embeddings required the multiplication of the input matrix with a dense, random matrix. Unfortunately, such general matrix multiplication can be very inefficient (and cumbersome) to carry out in a relational database.
>
> Our constructions, on the other hand, replace the inner product operations of matrix multiplication with view selection and aggregation (addition). Using, say, distribution (2) of Theorem 1.1 the k new coordinates are generated by independently performing the following random experiment k times: throw away 2=3 of the original attributes at random; partition the remaining attributes randomly into two parts; for each part, produce a new attribute equal to the sum of all its attributes; take the difference of the two sum attributes.
>
> http://users.soe.ucsc.edu/~optas/papers/jl.pdf
>

requires `n_components >= 4 log(n_samples) / (eps^2 / 2 - eps^3 / 3)` dimensions.



[[hashing]]
=== Hashing ===
// 
// The notion of "randomness" is highly ambiguous at best, philisophically ill-founded at worst. It's a bunch of ideas smushed together, two of which are (I'll explain) _unpredictable_ and _information-free_.
// (...explain unpredictable...) 
//
// remove distribution
// The distribution of byte sequences 
// http://en.wikipedia.org/wiki/Stochastic --

A good hash function does two things: produces _uniform output_, and _disperses nearby inputs_ (the "avalanche" effect).
This mixing means that in the rare case two objects have a colliding hash, they are even-yet-more-rarely likely to be similar as well. 

=== Features ===


that can be sensibly compared
  (air temperature and number of home runs are numeric and have a meaningful distance;
  flight ID or

In contrast, categorical features(The R language refers to them as _factors_).

// Make sure "Euclidean" is what we want

=== Similarity ===

* There are a few features.
* There are a dozen or a few dozen comparable features, and most of them have a value
* There are hundreds, thousands or millions of features, and most of them are zero or nearly zero:
  - in the bag-of-words representation of a document, every word in the entire corpus is a feature; the value might be a 1/0 indicating the word's presence or absence; or a measure of how often the word is used in the document (count, document frequency, or tf/idf, of which more later)
  - a problem with dozens of categorical variables might unpack to a problem with hundreds of comparable variables: census data (occupation; immigrant status; homeowner status; religion).


==== Jaccard Similarity ====

In the case where there are tons of features, it's often enough that the mere presence -- just being significantly larger than zero -- is enough to judge things as similar (or potentially similar) in that aspect.
If you're backpacking in the Yukon and you meet another traveller, you'll stop and share a drink: simply being a human being is similarity enough. Close to home, most people choose drinking buddies with similar politics, taste and so forth.

So the dumbest non-trivial way to evaluate "do they share the same topic" is
"do they use the same words". That is, consider all words mentioned at least once in either document. How many of them appear in the other document?
Since we're now just talking about presence or absence of common terms, we can use the 'Jaccard Similarity' of sets to quantify their similarity:

    JacSim[ A, B ] =  (items in A and B) / (items in A or B or both)
                   =  ||A intersect B|| / ||A union B||
//		   = TODO TeX goes here		     



=== MinHash ===


1. It's OK to compare features by presence/absence and not degree (that is, Jaccard similarity is appropriate)
2. The features are very sparse - most of them are zero.




By "similar" you might mean "potentially similar" -- you'll often use minhash to identify _potentially similar_ objects, followed a more robust similarity measure. If so, just pretend I'm saying "potentially similar".
 
Minhash uses one seemingly-blunt move to execute three data transformations at once.
. It's like watching your Grandmom truss a turkey: what you thought she did in one yank was really a combined pull-stuff-twist-extend-tuck that will take a half hour for you to recreate with guests coming over.


* Digests the feature name to a compact value: features are represented by a single machine word and not say a many-bytes-long character strings. This typically means faster comparisons, less memory usage, and less data sent to reducer.

* Truncating the digest 'folds' the feature space on itself, so that you can
  store aggregate information about features (eg counts) in a bounded number of buckets rather than an unknowable amount of RAM.

* "Randomly" permutes the feature names: the secret to how it approximates the Jaccard similarity


There are _two_ different hashings going on:

* a strong-mixing hash (eg murmur or MD5) to "consistently randomize" the features
* a locality-sensitive hash (eg modulo N) to fold the feature space


==== accuracy

even if the bucket for the term `also`  collides with the term `zephyr`,
  you're trusting that similarity is robust enough to shine through even without proper detection of `zephyr`.

=== Locality-sensitive hash

First, if two items are nearby, there is a high probability they hash to the same value

    Pr[ h(x) = h(y) ]    >=    P_1         for x, y such that |x-y| <= R

    "The probability        is above       for value within a 
    the hash functions      the chosen     radius R of each other
    are the same            limit 'P_1'

Second, if two items are far apart, there is a low probability they hash to the same value:

    Pr[ h(x) = h(y) ]    <=    P_2         for x, y such that |x-y| <= R

    "The probability        is *below*     for values *outside* a radius
    the hash functions      the chosen     `c*R` of each other (a multiple 
    are the same            limit 'P_2'    of the 'nearbyness' radius)

An example of an LSH is the Hamming distance using `j` arbitrary indices
  (example: fingerprint identification)

  


* http://www.stanford.edu/~ashishg/amdm/handouts/scribed-lec8.pdf[Notes from "Algorithms for Modern Data Models"] course by Ashish Goel
  



=== Brute-force k-Nearest Neighbors ===

Even if the baseball data isn't very big at 17,000 lines, brute-force comparison of each player to each other player requires nearly 150 million pairs. A walk in the park for Hadoop, but since it will give us something to compare against, let's take that walk.




=== Dimensionality Reduction ===

If you're taking the dot product,
it's OK to smush some of the dimensions together.

    u1*u2 + v1*v2 + w1*w2 + x1*x2 + y1*y2 + z1*z2

    (u1-v1)*(u2-v2) + (w1-x1)*(w2-x2) + (y1-z1)*(y2-z2)

    u1*u2 + v1*v2 + w1*w2 + x1*x2 + y1*y2 + z1*z2
       - (u1*v2 + v1*u2 + ) 

But remember: if the dot product is near one, most of the coordinates of the two vectors agree.

        < 0.5  0.2  0.1  0.0  0.1  0.1 >
        < 0.3  0.1  0.4  0.1  0.1  0.1 >

    Sum[  0.15 0.02 0.04 0.01 0.01 0.01 ] = 0.24

       
       def smushed(a) [a[0]-a[1], a[2]-a[3], a[4]-a[5]] ; end ;
       def smushed(a) normed([a[0]-a[1], a[2]-a[3], a[4]-a[5]]) ; end 
       def smushed(a) normed([a[0]+a[1], a[2]+a[3], a[4]+a[5]]) ; end


       def smushed(a) normed([ a[0]-a[1], a[2]-a[3], a[4]-a[5], a[3]-a[0], a[5]-a[2] ]) ; end 
       
       def dot_prod(a, b) a.zip(b).map{|ael, bel| ael.to_f * bel }.sum ; end ;
       def vec_len(a) Math.sqrt(dot_prod(a,a)) ; end ;
       def normed(a) vl = vec_len(a) ; a.map{|ael| ael.to_f / vl } ; end


       ## definitely no:  def smushed(a) [ (a[0]+a[1])*0.5, (a[2]+a[3])*0.5, (a[4]+a[5])*0.5 ] ; end


       rveca = [50,  2,  1,  0, 10,   1] ; rvecb = [30, 20, -4,  -1,  1,  1] ;
       rvecc = [40,  3,  3,  0, 15,   2] ; rvecd = [ 0,  0, 60,  20,  0,-10] ;
       veca = normed(rveca) ; vecb = normed(rvecb) ; vecc = normed(rvecc) ; vecd = normed(rvecd)
       all = [veca, vecb, vecc, vecd] ;


       [ [veca, vecb], [veca, vecc], [veca, vecd], [vecb, vecc], [vecb, vecd], [vecc, vecd] ].each{|v1, v2| puts "%7.4f\t%7.4f" % [dot_prod(v1, v2), dot_prod(smushed(v1), smushed(v2))] } ;





       

==== Refs ====

* http://www.slaney.org/malcolm/yahoo/Slaney2008-LSHTutorial.pdf[Locality-Sensitive Hashing for Finding Nearest Neighbors] by Malcolm Slaney and Michael Casey
* http://lingpipe-blog.com/2011/01/12/scaling-jaccard-distance-deduplication-shingling-minhash-locality-sensitive-hashi/
* http://www.scribd.com/collections/2287653/Locality-Sensitive-Hashing
* http://infolab.stanford.edu/~ullman/mmds/ch3a.pdf
* http://infolab.stanford.edu/~ullman/mining/2009/similarity2.pdf
* http://infolab.stanford.edu/~ullman/mining/2009/similarity1.pdf
* http://metaoptimize.com/qa/questions/8930/basic-questions-about-locally-sensitive-hashinglsh
* http://users.soe.ucsc.edu/~optas/papers/jl.pdf
* http://www.win-vector.com/dfiles/LocalitySensitiveHashing.pdf[An Appreciation of Locality Sensitive Hashing]
* http://www.cs.jhu.edu/~vandurme/papers/VanDurmeLallACL11.pdf[Efficient Online Locality Sensitive Hashing via Reservoir Counting]
* http://blog.smola.org/post/1130198570/hashing-for-collaborative-filtering

Johnson-Lindenstrauss Transform:

* https://www.cs.princeton.edu/~chazelle/pubs/stoc06.pdf
* http://ecee.colorado.edu/~fmeyer/class/ecen5322/ailon-chazelle2009.pdf[The Fast Johnsonâ€“Lindenstrauss Transform And Approximate Nearest Neighbors]
* http://scikit-learn.org/stable/auto_examples/plot_johnson_lindenstrauss_bound.html

Counting Streams (Count-Min-Sketch and friends):

* http://arxiv.org/pdf/0803.0473.pdf[Stream sampling for variance-optimal estimation of subset sums]

The split-apply-combine pattern in R: 

* 
* http://had.co.nz/reshape/paper-dsc2005.pdf
* 

==== Exercises ====

The approach we use here can be a baseline for the practical art of authorship detection in legal discovery, where a 



