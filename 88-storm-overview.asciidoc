One of the biggest changes to the practice of big data over the last year has been the rapid adoption of streaming data analytics. 
The obvious applications are extract/transform/load ("ETL") processes -- the automated version of chapter (TODO REF) on Data Munging -- and complex event processing ("CEP") -- the kind of real-time data handling that powers Wall Street and high-scale security monitoring. 

It's best instead to think of Stream Analytics as a way to query your data on the way _in_ to your datastore. A full-capability big data platform has all three of the following pieces:

* Stream Processing, to assemble data in simple context at write time;
* Queryable Datastore, to assemble data in simple context at read time;
* Batch Processing, to assemble data in global context 
 
The Storm+Trident data processing system, developed by Backtype and Twitter, is a remarkably elegant open-source project that's emerging as the dominant streaming analytics platform.

While the raw API of Storm+Trident is quite different from that of Hadoop, the interesting parts are the same. Wukong unifies the two, letting us concentrate on data and questions. Let's dive in.

=== Storm+Trident

Storm is the underlying framework that handles transport, messaging, process supervision and so forth. It guarantees reliable processing of each record using a brilliantly simple scheme that will scale to millions of records without requiring its own massive data flow for bookkeeping. 

Trident builds on Storm to provide exactly once processing and transactional  
* layer on the flow DSL
* some primitive aggregators


==== Orchestration and Transformation

* Spouts and Bolts, to produce and transform records respectively;
* Topologies, to orchestrate the grouping and combining of data streams

==== Reliability and Guaranteed Processing

The central challenge of high-scale stream analytics is this: the amount of metadata required to coordinate and process the data reliably can itself grow to such a volume that it becomes difficult to coordinate and process reliably. (TODO: wow needs rewording)

A distributed framework can provide one of the following guarantees:

* "best-effort": 
* "at least once": every record will be successfully processed at least once. If your data 
* "exactly-once": every record is successfully processed, and every unsuccessful attempt can be reliably invalidated or retracted

A surprising number of tasks require only best-effort or at-least-once processing
But exactly-once processing is essential for analytics processes: counting distinct records, 

=== Trident Lifecycle

==== Coordinator

* Master coordinator is secretly the spout
* at trident batch delay period, will emit a transaction tuple
* it has a serially incrementing transaction ID, kept forever even across restarts.
* (We're only going to talk about Opaque Transactional topologies; more on that later)

==== Spout

spout emits batches; these succeed or fail as a whole. (That's because they're part of the Storm tuple tree of the coordinator's seed tuple).

==== Processors

* tuples in batches are freely processed in parallel and asynchronously unless there are barriers (eg the state after the group by)
* In this case, Processor emits aggregable records

==== Group-by

==== Transactional State

* the state doesn't ask the cache to fetch until it has a whole batches' worth of records to hand over. This is trident logic, not storm.
* Those "aggregables" are reduced into rolled-up aggregates. So you might have 2500 inbound records that result in 900 distinct aggregates. (If you had eight aggregables [A, A, C, A, B, D, B, A] you would get four partial aggregates {A: 4, B: 2, C: 1, D: 1}. 
* It's clever about doing partial aggregates ("algebraic" reducers).


* It looks in the cache for the old total count. Anything that isn't there it fetches from the database. This lets you do efficient batch requests, a huge scalability boon.
* Once the cache is fresh, it determines the next aggregated value and writes it to the cache and to the DB, then ack()s the batch (all the tuples in the batch, really).
* If a batch had 900 aggregates, and it had prior counts for 250 of them, then it will _read_ 650 records and _write_ 900. It always does a put for every new observed count.

* Â¡Note!: The database writes do *not* have to be transactional. It's the whole thing -- the whole batch, end-to-end -- that has to have transactional integrity, not just the DB code.

* __Bolt__ -- topology-level object.

  - contract:
    - `execute` is called
    - you may call `emit` zero one or many times,
    - and then you must call either `ack` or `fail`
    - so, execute method _must_ be synchronous (blocking calls). No fair suspending yourself and returning from execute for some later ackness or failness. That's Storm's job.
      - TODO: verify.


__Physical-level objects__

* __Supervisor__
  - hosts worker
  - has many workers
* __Worker__
  - has many executors, belongs to supervisor
  - role:
    - hosts zmq sockets
    - accepts inbound tuples from other workers (worker receive queue)
    - dispatches outbound tuples to other workers (worker transfer queue)
    - (other stuff)

* __Executors__
  - belongs to executor; has one bolt/spout
  - role:
    - accepts inbound tuples (executor receive queue)
    - dispatches outbound tuples (executor send queue)
  - each executor is one single thread
   - calls tasks serially
* __Tasks__ --
  - belongs to executor; has one bolt/spout
  - physical expression of the bolt or spout
  - in Storm, can set many tasks per executor -- when you want to scale out (TODO: verify). (in Trident, left at one per; TODO: can this be changed?)

===== Ensuring Transactional reliability

Let's say for transaction ID 69 the old aggregated values were `{A:20, B: 10, C: 1, D: 0}`, and new  aggregated values were `{A: 24, B: 12, C: 2, D: 1}`. 

It stores (TODO: verify order of list):

   {A: [24, 20, 69], B: [12, 10, 69], C: [2, 1, 69], D: [1, 0, 69]}

If I am processing batch 

Since this is a _State_, you have contractual obligation from Trident that batch 69 will *not* be processed until and unless batch 68 has succeeded. 

So when I go to read from the DB, I will usually see something like

   {A: [20, ??, 68], B: [10, ??, 68], C: [1, ??, 68]}

I might instead however see

  {A: [??, 20, 69], B: [??, 10, 69], C: [??, 1, 69], D: [??, 0, 69]}

This means another attempt has been here: maybe it succeeded but was slow; maybe it failed; maybe _I_ am the one who is succeeding but slow. In any case, I don't know whether to trust the _new_ (first slot) values for this state, but I do know that I can trust the prior (second slot) values saved from batch 68. I just use those, and clobber the existing values with my new, correct counts.

===== Kinds of State

* non-transactional: batching behavior only
* transactional: exactly once; batches are always processed in whole
* opaque transactional: all records are processed, but might not be in same batches

==== Numerology

The following should be even multiples:

* `N_w` workers per machine. (one if you're only running one topology)
* `N_spouts` per
  - `N_partitions_per_spout` -- even number of partitions per spout
  
* Don't change multiplicity lightly
  - it will route directly
  - don't really understand how/when/why yet


* Parallelism hint is a hint --
  - can get more never less (TODO: verify)

==== Tuple handling internals

==== Queues

* executor send buffer
* executor receive buffer
* worker receive buffer
* worker transfer buffer


=== Why Storm+Trident is bigger than it looks


*  Operational decoupling:
* Latency Tolerance:
* Reliability Glue:
* Transport Agnosticism:
* Distributed Programming without quantum mechanics

How do you make a program that will run forever? Joe Armstrong, the inventor of Erlang, identifies these six key features: 
Isolation; Concurrency; Failure Detection; Fault Identification, Live Code Upgrade; Stable Storage
Storm+Trident provides all six, 
