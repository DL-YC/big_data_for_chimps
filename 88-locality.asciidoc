* Option one: Parallel GC (`-XX:+UseParallelGC`) with `-XX:UseAdaptiveSizePolicy -XX:+PrintAdaptiveSizePolicy`. If too much latency, look at
* Option two: CMS (`-XX:+UseConcMarkSweepGC`)
* Option three: G1 (`-XX:+UseG1GC` with `-XX:MaxGCPauseMillis=` to set the target time).
* `-XX:ParallelGCThreads` and `-XX:ParallelCMSThreads` specify the number of parallel CMS threads.
* Other flags that affect performance include `-XX::+UseCompressedOops`, `-XX:+UseLargePages`, `-XX:LargePageSizeInBytes`, `-XX:+UseNUMA`, `-XX:+AggressiveOpts`, `-XX:AggressiveHeap`, `-XX:+UseBiasedLocking`, `-XX:+DoEscapeAnalysis`, `-XX:+AlwaysPreTouch`
* Useful for monitoring are -XX:+PrintCommandLineFlags and -XX:+PrintFlagsFinal.

Proximity - adjacency
Context - reshape - pivot

=== Three legs of the big data stack

In early 2012, my company (Infochimps) began selling our big data stack to enterprise companies. At first, clients came to us with the word Hadoop on their lips and a flood of data on their hands -- yet what consistently emerged as the right solution was an initial implementation using streaming data analytics into a scalable datastore, and a follow-on installment of Hadoop once their application reached scale. From nowhere on their radar last year, we now hear requests for Storm by name. I've seen Hadoop's historically fast growth from open-source product with momentum in 2008 to foundational enterprise technology in 2013, and can attest that the rate of adoption for streaming analytics (and Storm+Trident in particular) looks to be even faster. 

It's become clear that a big data application platform should have three legs: streaming analytics, to process records as they are created; one or more scalable databases, for processing records as they are consumed; and batch processing, for results that require the full dataset. 

The workflow described in this book unifies and simplifies the streaming data and Hadoop frameworks without limiting their fundamental power. We use wukong (Ruby) for direct transformations of records, and high-level DSLs (Pig for Hadoop, Trident for streaming) to orchestrate structural transformations of datasets. This lets the data scientist focus on their data and problem domain, not on the low-level complexity of the half-dozen APIs otherwise involved.

For both Storm+Trident and Hadoop, our intent is to demonstrate

1. a pattern language for orchestrating the global structural changes efficiently,
2. practical instruction and street-fighting techniques for data science as it's done
3. real-data, real-problem case studies to demonstrate the above

These overlap far more than you might expect, and the hard part reamins the same: understanding 'locality' and how to deal with data dispersed across hundreds of machines. (In fact, a map-reduce job is simply a certain type of especially bursty dataflow.) Storm+Trident introduces a second DSL for describing that process, but the hard part is what script to write, not the writing of the script. The chapter on statistics will simplify in scope but describe both global and streaming algorithms for statistical summaries. The chapter on time series algorithms will be scaled back to simply being anomaly detection ("trending topics" detection), and presented using Storm+Trident. Third, we'll repurpose material from the log processing and the machine learning chapters to demonstrate an end-to-end big data application that combines Hadoop, Storm+Trident and HBase to make efficient online recommendations for a large-scale web application. The major additions are a chapter on tuning and on the internals of Storm+Trident.

=== Storm

==== Reliability

Storm provides "at least once" processing of data. 

Storm uses an incredibly elegant strategy to track the successful completion in whole of a tuple tree.
The acker task tracks a check 
(TODO: verify -- attempt id combines the tuple's id (`messageId`) and task id)
For each tuple tree. Each execute attempt id is XORed onto the checksum on execute and again on completion.

The XOR function is commutative: `A ^ B == B ^ A`, and the XOR of a number with itself is zero. So if the acker got the events `A_beg, B_beg, A_end, C_beg, C_end, B_end`, the resulting checksum would be `A ^ B ^ A ^ C ^ C ^ B`, which is the same as `A ^ A ^ B ^ B ^ C ^ C` -- which collapses to zero. Without proving the second point, 

* If the tuple tree is processed successfully, each attempt id will appear twice and so the sum must be zero if successful
* If the tuple tree is not -- if some one or more tuples are incorrectly acked -- it is exceedingly unlikely the the checksum will be zero.

This lets Storm process millions of tuples in very little memory or processor.

=== Trident

* Website request vs customer info
* Tweet vs followers
* Activity content vs geo context
* Trade request - risk analysis - hedge - verification
* Document security - patterns of access

=== Locality Models
* RPC - RPC
* Client-server data store
* Streaming Analytics
* Fabric (VCD)
* Batch

* Latency
* Throughput
* Tempo -- how often does data change?
* Size -- how large is record?
* Access control -- security; API rate limits
* Data model -- your web log hit (with path, response time, HTTP status code, etc) is my sales lead.

=== Lambda Architecture

* _Fast data_: recorded live, updates allowed with partial locality or denormalized data
* _Slow data_: gold data, using global data, full answer.

=== Example lambda architecture: online pagerank

* Start with stable pagerank.
* When a new node is discovered, just "borrow" a notional pagerank allocation from its neighbors
* Don't worry about any beyond immediate locality
* Later, batch job re-settles the graph.
* Pagerank calculation is idempotent: within reason, any perturbed input will settle out.

=== locality in stream

* GroupBy / Partitioned aggregates
* DRPC
* Denormalized remote data request
* Hash join -- hold a cached version of table and decorate

==== Why can you get away with 

Storm/Trident has buffering and throttling mechanisms built in

Hadoop is designed to drive all system resources to their full limit until the fundamental limiting resource is encountered. 
