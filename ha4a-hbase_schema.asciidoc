== HBase Data Model ==

Space doesn't allow treating HBase in any depth, but it's worth equipping you with a few killer dance moves for the most important part of using it well: data modeling. It's also good for your brain -- optimizing data at rest presents new locality constraints, dual to the ones you've by now mastered for data in motion.  So please consult other references (like "HBase: The Definitive Guide" (TODO:reference) or the free file:///data/docs/hbase.apache.org/book.html#quickstart[HBase Book] online), load a ton of data into it, play around, then come back to enjoy this chapter.

=== Row Key, Column Family, Column Qualifier, Timestamp, Value ===

You're probably familiar with some database or another: MySQL, MongoDB, Oracle and so forth. These are passenger vehicles of various sorts, with a range of capabilities and designed for the convenience of the humans that use them. HBase is not a passenger vehicle -- it is a big, powerful dump truck. It has no A/C, no query optimizer and it cannot perform joins or groups. You don't drive this dump truck for its ergonomics or its frills; you drive it because you need to carry a ton of raw data-mining ore to the refinery. Once you learn to play to its strengths, though, you'll find it remarkably powerful.

Here is most of what you can ask HBase to do (roughly in order of efficiency):

1. Given a row key: get, put or delete a single value into which you've serialized a whole record.
2. Given a row key: get, put or delete a hash of column/value pairs, sorted by column key.
3. Given a key: find the first row whose key is equal or larger, and read a hash of column/value pairs (sorted by column key).
4. Given a row key: atomically increment one or several counters and receive their updated values.
5. Given a range of row keys: get a hash of column/value pairs (sorted by column key) from each row in the range. The lowest value in the range is examined, but the highest is not.
6. Feed a map/reduce job by scanning an arbitrarily large range of values.

That's pretty much it! There are some conveniences (versioning by timestamp, time-expirable values, and a type of vertical partitioning known as column families); some tunables (read caching, fast rejection of missing rows, and compression); and some advanced features, not covered here (query filters, and a kind of stored procedures/stored triggers called coprocessors). For the most part, however, those just help optimize the access patterns listed above.

A good HBase data model is "designed for reads". Your goal is to make _one read per customer request_ (or as close as possible). If you do so, HBase will yield response times on the order of 1ms for a cache hit and 10ms for a cache miss, even with billions of rows and millions of columns.

[[hbase_no_yuo]]
[NOTE]
===============================
Here's a partial list of features you do _not_ get in HBase:

* efficient querying or sorting by cell value
* group by, join or secondary indexes 
* text indexing or string matching (apart from row-key prefixes)
* arbitrary server-side calculations on query
* any notion of a datatype apart from counters; everything is bytes in/bytes out
* auto-generated serial keys

Sometimes you can partially recreate those features, and often you can accomplish the same _tasks_ you'd use those features for, but only with significant constraints or tradeoffs. (You _can_ pick up the kids from daycare in a dump truck, but only an idiot picks up their prom date in a dump truck, and in neither case is it the right choice). 

More than most engineering tools, it's essential to play to HBase's strengths, and in general the simpler your schema the better HBase will serve you. Somehow, though, the sparsity of its feature set amplifies the siren call of even those few features. Resist, Resist. The more stoicly you treat HBase's small _feature_ set, the better you will realize how surprisingly large HBase's _solution_ set is.
==================================

=== Keep it Stupidly Simple ===

Let's sketch the implementation of an autocomplete API on Wikipedia page titles, an example that truly plays to HBase's strengths. As a visitor types characters into a search bar, the browser will request a JSON-encoded list of the top 10 most likely completions for that prefix. Responsiveness is essential: at most 50 milliseconds end-to-end response time. Several approaches might spring to mind, like a range query on titles; a prefix query against a text search engine; or a specialized "trie" datastructure. HBase provides a much stupider, far superior solution.

Instead, we'll enumerate every possible completion footnote:[First, join on the pagerank table (see TODO: ref) to attach a "prominence" to each page; we'll keep only the top 10 by rank for each prefix. Next, write a map-reduce job: the mapper takes each title and emits the first three, four, five, up to say twelve characters along with the pagerank. Have hadoop use the prefix as partition key, and the prefix-rank as a descending sort key. Now on each new prefix group, capture up to ten completions -- that's the return value.]. This blows the dataset into the billion-row range, but it makes each request a highly cache-efficient key/value lookup. Given an average title length of (TODO: insert numbers), the full completion set weighs in at "only" (TODO: numbers) rows and XXX raw data size -- a walk in the park for HBase. 

What will we store into HBase? Your first instinct might be to store each of the ten titles, each in its own cell. Reasonable, but still too clever. Instead, serialize the full JSON-encoded response as a single value. This minimizes the cell count (memory- and disk-efficient), lets the API front end put the value straight onto the wire (speed and lines-of-code efficient), and puts us in the most efficient access pattern: single row, single value.

[[hbase_schema_autocomplete]]
.Autocomplete HBase schema
|=======
|table             | row key    	  | column family  | column qualifiers | value                 | options
| title_autocomp   | `prefix`             | 'j'             | `-`                | JSON-encoded response | `VERSIONS => 1, BLOOMFILTER => 'ROW', COMPRESSION => 'SNAPPY'`
|=======

==== Help HBase be Lazy ====

In the autocomplete example, many requests will be for non-existent rows (eg "hdaoop"). These will of course be cache misses (there's nothing to cache), making the queries not just useless but also costly. Luckily, there's a specialized data structure known as a "Bloom Filter" that lets you very efficiently test set membership. If you explicitly enable it footnote:[A bug in the HBase shell may interfere with your ability to specify a bloom filter in a schema -- the https://issues.apache.org/jira/browse/HBASE-3086[HBASE-3086 bug report] has a one-line patch that fixes it.], HBase will capture all row keys into a Bloom Filter; on each request, it will quickly make sure it's worth trying to retrieve a value before doing so.

==== Row Locality and Compression ====

There's another reason HBase is a great match for this problem: row locality. HBase stores all rows in sorted order on disk, so when a visitor has typed *+chim+*, the rows for `chime` and `chimp` and so forth are nearby on disk. Whatever next character the visitor types, the operating system is likely to have the right block hot in cache. 

That also makes the autocomplete table a candidate for block-level compression. Compression drives down the data size, which of course economizes disk capacity -- more importantly, though, it means that the drive head has less data to seek past, and the IO bus has less data to stream off disk. Row locality often means nearby data elements are highly repetitive (definitely true here), so you realize a great compression ratio. There are two tradeoffs: first, a minor CPU hit to decompress the data; worse though, that you must decompress blocks at a time even if you only want one cell. In the case of autocomplete, row locality means you're quite likely to use some of those other cells. 

=== Range Lookup ===

If you recall from (TODO ref server logs chapter), the Geo-IP dataset stores information about IP addresses a block at a time.

* _Fields_: IP address, ISP, latitude, longitude, quadkey
* _query_: given IP address, retrieve geolocation and metadata with very low latency

[[hbase_schema_ip_geo]]
.IP-Geolocation lookup
|=======
|table  	| row key       	  | column families  | column qualifiers | versions  | value
| ip    	| `ip_upper_in_hex`       | field name       | `-`               | none	  |
|=======

Store the _upper_ range of each IP address block in hexadecimal as the row key. To look up an IP address, do a scan query, max 1 result, on the range from the given ip_address to a value larger than the largest 32-bit IP address. A get is simply a scan-with-equality-max-1, so there's no loss of efficiency here.

Since row keys are sorted, the first value equal-or-larger than your key is the end of the block it lies on. For example, say we had block "A" covering `50.60.a0.00` to `50.60.a1.08`, "B" covering `50.60.a1.09` to `50.60.a1.d0`, and "C" covering `50.60.a1.d1` to `50.60.a1.ff`. We would store `50.60.a1.08 => {...A...}`, `50.60.a1.d0 => {...B...}`, and `50.60.a1.ff => {...C...}`. Looking up `50.60.a1.09` would get block B, because `50.60.a1.d0` is lexicographically after it. So would `50.60.a1.d0`; range queries are inclusive on the lower and exclusive on the upper bound, so the row key for block B matches as it should.

As for column keys, it's a tossup based on your access pattern. If you always request full rows, store a single value holding the serialized IP block metadata. If you often want only a subset of fields, store each field into its own column.

=== Geographic Data ===

[[hbase_schema_geographic_data]]
.Server logs HBase schema
|=======
|table             | row key    	  | column families    | column qualifiers | versions  | value
| tile_regions     | `quadkey`   	  | (region type)      | `region_name`     | none      | Geo-JSON encoded path
| regions          | `region_name`   	  | (region type)      | (field name)      | none      | Value of field
|=======

Given an arbitrary spatial extent, we want to retrieve all regions, all regions of a given type (country, census block, ...), or the parts of a specific region.

==== Multi-scale indexing ====

At some point of zoom out, there will simply be too much data
Compute a summary, and store it under the truncate key -- i.e. store the rollup of '012312000' to '012312333' under '012312'.


=== Simple Table ===

Now let's visit the gift shop and see a table with all the options. The urge to kit out your dump truck with fuzzy dice and a spoiler is inescapable, and for small tables there's no harm in making them ergonomic.

==== Airport Metadata ====

The airport info table is dense: pretty much every row has a value for every column.

* _Fields_: three primary identifiers (IATA, ICAO, FAA),
* _Queries_: given airport identifier, get record; get airports contained in a geo region (quadtile)
* _You cannot_: look up an airport by anything but its identifiers or location.

[[hbase_schema_airport_metadata]]
.Airport Metadata
[width="100%"]
|=======
| table  	| row key       	  | column families  | column qualifiers | versions  | value
| airports	| `{id_type}{identifier}` | each field name  |		      |		  |
| geo_airports	| `quadkey-airport_id`	  |		     |		      |		  |
|=======

The "airports" table is so small that our choices don't affect performance. If it were many many times larger, there would be three plausible choices:

* each field is its own column family, with a null qualifier
* there is one column family, and the field name is the column qualifier
* there one column, and the entire record is serialized into its value. 

The answer comes down to the pattern of access from your application. If you only access a subset of rows, 

Since the HBase data model may well change as you learn what constraints the production workload imposes -- and since the data model may not look much like your application's data model anyway -- it's a very good idea to put an abstraction layer, no matter how thin, between your application and the datastore.

==== Airport Timezone ====

Here's a great excuse to use HBase's version feature.
First, be sure to set the VERSIONS option to a very large number when you create the `tzoff` column family.
Next, we will store the TZ offset at the exact timestamp marking the beginning of its rule.
When you do a query, specify that you want one value, with the max timestamp set to the time of the event at that airport.
TODO: local time? utc time? which way must we go.

=== Wikipedia: Corpus and Graph ===

[[hbase_schema_corpus]]
.Wikipedia HBase schema
|=======
|table              | row key		   | family | qualifier | value    | 
| articles          | `page_id`             | `t`   |            | text    | 
| article_versions  | `page_id`             | `t`   |            | text    | timestamp: updated_time
| article_revisions | `page_id-revision_id` | `v`   |            | text, user_id, comment
| categories        | `category-page_id`    | `c`   |            | 
| redirects         | `bad_page_id`         | `r`   |            | `proper_page_id`
|=======

==== Graph Data ====

Just as we saw with Hadoop, there are two sound choices for storing a graph: as an edge list of `from,into` pairs, or as an adjacency list of all `into` nodes for each `from` node.

[[hbase_schema_wikipedia_pagelinks]]
.HBase schema for Wikipedia pagelink graph: three reasonable implementations
|=======
|table             | row key		   | column families | column qualifiers | value   | options
| page_page        | `from_page-into_page` | `l` (link)       | (none)            | (none)  | `bloom_filter: true`
| page_links       | `from_page`           | `l` (links)      | `into_page`       | (none)
| page_links_ro    | `from_page`           | `a` (adj. list)  | (none)            | serialized adjacency list
|=======

If we were serving a live wikipedia site, every time a page was updated I'd calculate its adjacency list and store it as a static, serialized value. 

For a general graph in HBase, here are some tradeoffs to consider:

* The pagelink graph never has more than a few hundred links for each page, so there are no concerns about having too many columns per row. On the other hand, there are many celebrities on the Twitter "follower" graph with millions of followers or followees. You can shard those cases across multiple rows, or use an edge list instead.
* An edge list gives you fast "are these two nodes connected" lookups, using the bloom filter on misses and read cache for frequent hits.
* If the graph is read-only (eg a product-product similarity graph prepared from server logs), it may make sense to serialize the adjacency list for each node into a single cell. You could also run a regular map/reduce job to roll up the adjacency list into its own column family, and store deltas to that list between rollups.

=== Web Logs: Rows-As-Columns ===

Assume a high-volume eCommerce website: 2 million unique daily visitors, causing 100 M requests/day on average (4000 requests/second peak) from 20-40 servers, and about 600 bytes per log line. Over a year, that becomes about 40 billion records and north of 20 terabytes of raw data. Show that to most databases and they will crumble. Show it to HBase and it will ask you to store it multiple times over (and we will).

* Queries:
  - Visitor paths: the pages visited on the way to a purchase, including external _referer_ sites, _search terms_ entered, items _added to cart_, and finally _conclusion of purchase_.
  - Abuse: anomalously large numbers of requests coming from single IP addresses
  - Product similarity: pages visited in common during a session.

* Fields: `ip_address`, `cookie` (a unique ID assigned to each visitor),
  - `path`
  - `referer_url`, and `referer_int` showing if the referer was internal (1) or external (0).
  - `status_code` (success or failure of request) `duration` (time taken to render page)

We'll further augment with these fields:

* `timestamp_rev`, a "reverse timestamp" -- INT_MAX - time.to_i. This means that the most recent visit for that site sorts first in column order. http://hbase.apache.org/book.html#reverse.timestamp

[[hbase_schema_server_logs]]
.Server logs HBase schema
|=======
|table             | row key    	  | family         | qualifier | value           | options
| visits           | `cookie-timebucket`  | 'r' (referer)   | `referer`     | - 		 |
| visits           | `cookie-timebucket`  | 's' (search)    | `term`        | - 		 |
| visits           | `cookie-timebucket`  | 'p' (product)   | `product_id`  | - 		 |
| visits           | `cookie-timebucket`  | 'z' (checkout)  | `cart_id`     | `{product_ids}` |
| cookie_urls      | `cookie`             | 'u' (url)       | `-`           |		 |
| ip_tbs           | `ip-timebucket`   	  |        	    |              |		 |
|=======

==== Column Families ====

To understand users' path through the site, 

External referer, Search term, cart action

===== External Referer =====

When storing URLs, it's common to use the "domain-reversed" url (eg "org.apache.hbase/book/quickstart.html"), where the hostname segments are placed in reverse order. This means that pages served from different hosts within the same organization ("org.apache.hbase" and "org.apache.kafka" and so forth) are ordered adjacently.

==== Atomic Counters ====

We'd like to track, for each visitor, the URLs they view with the number of times viewed. HBase offers _atomic counters_, an exceptionally important feature.

In a distributed system, it does not work to read a value from the database, add one to it, and write it back -- some other agent elsewhere may be busy doing the same, or your write may not make it to the server until it's well out of date. Without native support for counters, this simple process requires locking, retries, or other complicated/expensive machinery.  Hbase lets you issue a single 'incr' command, with the guarantee that it will be applied consistently and that you will receive the new value in response.

That makes the visitor-URL tracking trivial. Build a table called `cookie_url`, with a column family `"u"`. On each page view, simply increment the number of times the url has been seen: `count = incr(table: "cookie_url", row: cookie, col: "u:#{url}")`. You don't have to initialize the cell; if it was NULL, HBase will treat it as having a count of zero. 

==== Most-Frequent URLs ====

We'd also like to track, for each visitor, the most _frequent_ URLs they visit. Locality issues typically make queries like this impractical: you need to know the counts for all the URLs to know which is largest. In this case, however, there's a filthy hack that will let you track the single most frequent element.

What we're going to do is abuse HBase's timestamp feature. Add a column family `c` having `VERSIONS: 1` to the `cookie_stats` table. On each view, we'll do two writes:

1. As before, increment the counter for that URL: `count = incr(table: "cookie_url", row: cookie, col: "u:#{url}")`. The return value of the call has the updated count.
2. Store the URL in the `cookie_stats` table, but use a _timestamp equal to that URL's count_ (not the current time) --  `put("cookie_stats", row: cookie, col: "c", timestamp: count, value: url)`.

To find the value of the most-frequent URL for a given cookie, do a `get(table: "cookie_stats", row: cookie, col: 'c')`. HBase will return the "most recent" value, namely the one with the highest timestamp, which means the value with the highest count. Although we're constantly writing in values with lower counts, HBase ignores them on queries and eventually compacts them away.

==== Most-Recent URLs ====

We'd like to track, for each visitor, the five most recently-viewed products. In the `cookie_stats` table, add a column family `r` having `VERSIONS: 5`. Now each time the visitor loads a product page,

If you can't tolerate

There's a few ways to help HBase intelligently skip data or lighten its burden.

HBase store files record the timestamp range of their contained records. If your request is limited to values less than one hour old, HBase can ignore all store files older than that. The pattern with which HBase compacts its store files makes this especially convenient.

==== Rollup columns ====

HBase is a database for "billions of rows and millions of columns".

A timestamped metric table like this _writes by the column_ but _reads by the row_.

=== Row Locality ===

Row keys determine data locality. When activity is focused on a set of similar and thus adjacent rows, it can be very efficient or very problematic.

==== adjacency is good ====

Most of the time, adjacency is good (hooray locality!). When common data is stored together, it enables
  - range scans: retrieve all pageviews having the same path prefix, or a continuous map region.
  - sorted retrieval: ask for the earliest entry, or the top-`k` rated entries
  - space-efficient caching: map cells for New York City will be much more commonly referenced than those for Montana. Storing records for New York City together means fewer HDFS blocks are hot, which means the opeerating system is better able to cache those blocks.
  - time-efficient caching: if I retrieve the map cell for Minneapolis, I'm much more likely to next retrieve the adjacent cell for nearby St. Paul. Adjacency means that cell will probably be hot in the cache.

==== adjacency is bad ====

If _everyone_ targets a narrow range of keyspace, all that activity will hit a single regionserver and your wonderful massively-distributed database will limp along at the speed of one abused machine.

This could happen because of high skew: for example, if your row keys were URL paths, the pages in the `/product` namespace would see far more activity than pages under `laborday_2009_party/photos` (unless they were particularly exciting photos). Similarly, a phenomenon known as Benford's law means that addresses beginning with '1' are far more frequent than addresses beginning with '9' footnote:[A visit to the hardware store will bear this out; see if you can figure out why. (Hint: on a street with 200 addresses, how many start with the numeral '1'?)]. In this case, file:///data/docs/hbase.apache.org/book.html#important_configurations[managed splitting] (pre-assigning a rough partition of the keyspace to different regions) is likely to help.

Managed splitting won't help for http://ikaisays.com/2011/01/25/app-engine-datastore-tip-monotonically-increasing-values-are-bad/[timestamp keys and other monotonically increasing values] though, because the focal point moves constantly. You'd often like to spread the load out a little, but still keep similar rows together. Options include:

* swap your first two key levels. If you're recording time series metrics, use `metric_name-timestamp`, not `timestamp-metric_name`, as the row key.
* add some kind of arbitrary low-cardinality prefix: a server or shard id, or even the least-significant bits of the row key. To retrieve whole rows, issue a batch request against each prefix at query time.


DRAFT

DRAFT -- ignore below

DRAFT


==== Vertical Partitioning (Column Families) ====

Suppose that after releasing the autocomplete API, we find that a sizeable minority of developers want to consume pre-baked HTML rather than the existing (and still-popular) JSON response. No request returns both HTML and JSON, Instead, we'll store each response type in its own _column family_ in the autocomplete table. 
The pattern of access and data size are similar for each, but 
It might even be reasonable to put them in different tables. 

=== Feature Set review ===

* **TTL

* Atomic counters: accumulate a numeric value, guaranteed consistent even if multiple clients simultaneously update it
* TTL ("Time to Live"): an optional amount of time, after which values are expired.

* Versioning by timestamp
* Column Families

* read caching
* Bloom filters fast rejection of missing rows
* Block-level compression
* Row-level compression

* query filters: impose server load, 
* and a kind of stored procedures/stored triggers called coprocessors). Here's a partial list of things you do _not_ get:


=== "Design for Reads" ===

HBase stores data in cells, scoped like this:

* Table -- a hard partition of data. Tables are stored, partitioned and optimized in isolation.
* Row Key -- the primary key for a record. Row contents are stored together, sorted by row key.
* Column Key -- indexed elements of a row, in the form `column_family:column_qualifier` (the qualifier is optional).
  - Column Family -- coarse-grained sub-partition of a row. You must declare the column family in advance. There are several options (like number of versions) <remark>TODO: check</remark> you can set independently per column family.
  - Column Qualifier -- the arbitrary remainder of a column key;
* Value -- the contents you'd like to store, anything or nothing.

Table names and column familty names must be defined in advance, and their names may only contain printable characters (I recommend only using `[a-z_][a-z0-9_]*`). Everything else is bytes in / bytes out, exactly as issued.


* Avoid having more than a handful of column families on any high-performance table, especially if their patterns of write access are distinct.
* Avoid having more than a few million columns per row.

* Column families
  - always specify the `versions`: by default it's 3, and you almost always want 1 or a value you've thought very carefully about
  - Don't use more than two or three column families for a high-impact table; all of them have to keep pace with the most-heavily-used one.
* Use short row and column names. _Every_ cell is stored with its row, column, timestamp and value, every time. (trust the HBase folks: this is the Right Thing).
  - even still, fat row names (larger than their contents) often make sense. If so, increase the block size so that table indexes don't eat all your RAM.

* Keys should be space-efficient. Use _very_ short names for column families ('u', not 'url'). Don't be profligate with size of column keys and row keys on huge tables: a binary-packed SHA digest of a URL is more efficient than its hex-encoded representation, which is likely more efficient than the URL itself. However, if that bare URL will let you efficiently index on sub-paths, use a bare URL. For another example, we gladly waste 6 bits of every byte in a quadkey, because it lets us do multi-scale queries.
* Keys should be properly encoded and sanitized
  - HBase stores and returns arbitrary binary data, unmolested.

* All sorting is _lexicographic_: beware the "derp sort". Given row keys 1, 2, 7, 12, and 119, HBase stores them in the order 1, 119, 12, 2, 7: it sorts by the most significant (leftmost) byte first.
  - zero-pad decimal numbers, and null-pad binary packet numbers. Suppose a certain key ranged from 0 to 60,000; you would zero-pad the number 69 as `00069` (5 bytes); the null-padded version would have bytes `00 45` (2 bytes).
  - annoyingly, `+` sorts less than `-`, so `+45` precedes `-45`. However, `
  - reverse timestamp

* Timestamps let HBase skip HStores

* Always set timestamps on fundamental objects. Server log lines, tweets, blog posts, and airline flight departures all have an intrinsic timestamp of occurrence, and they are all "fundamental" objects, not assertions derived from something else.  In such cases, always set a timestamp.  In contrast, the "May 2012 Archive" page of a blog, containing many posts, is not fundamental; neither is an hourly cached count of server errors. These are _observations_, correct at the time they're made -- so that observation time, not the intrinsic timestamp

* make sure you set the VERSIONS when you create the table+column family
* Once you know your access patterns and can test the response under load, consider enabling compression. RECORD compression works best when you have fat rows (lots of columns) and you typically access the full row. There are so many tradeoffs at play, however, that you really need to just try it. Luckily, Hadoop is sitting right there ready to cross-load your tables.



.Composite Keys
NOTE notation -- HBase makes heavy use of composite keys (several values combined into a single string). We'll describe them using
* quote marks (`"literal"`) to mean "that literal string"
* braces `{field}` mean "substitute value of that field, removing the braces"
* and separators, commonly `:`, `|` or `-`, to mean "that character, and make damn sure it's not used anywhere in the field value".

HBase is a database for storing "billions of rows and millions of columns"

=== References ===

* I've drawn heavily on the wisdom of http://hbase.apache.org/book.html[HBase Book]

* Thanks to Lars George for many of these design guidelines, and the "Design for Reads" motto.

* http://helpmetocode.blogspot.in/2012/04/commands-available-on-hbase-shell.html[HBase Shell Commands]

* http://www.slideshare.net/larsgeorge/hbase-advanced-schema-design-berlin-buzzwords-june-2012[HBase Advanced Schema Design] by Lars George

* http://www.quora.com/What-are-the-best-tutorials-on-HBase-schema


* encoding numbers for lexicographic sorting:
  - an insane but interesting scheme: http://www.zanopha.com/docs/elen.pdf
  - a Java library for wire-efficient encoding of many datatypes: https://github.com/mrflip/orderly
