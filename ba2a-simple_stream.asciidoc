== The Stream ==

==== Chimpanzee and Elephant Start a Business ====

As you know, chimpanzees love nothing more than sitting at typewriters processing and generating text. Elephants have a prodigious ability to store and recall information, and will carry huge amounts of carge with great determination. The chimpanzees and the elephants realized there was a real business opportunity from combining their strengths, and so they formed the Chimpanzee and Elephant Data Shipping Corporation.

They were soon hired by a publishing firm to translate the works of Shakespeare into every language. 
In the system they set up, each chimpanzee sits at a typewriter doing exactly one thing well: read a set of passages, and type out the corresponding text in a new language. Each elephant has a pile of books, which she breaks up into "blocks" (a consecutive bundle of pages, tied up with string). 

==== A Simple Streamer ====

We're hardly as clever as a language-chained chimpanzee, but even we can translate text into Pig Latin. For the unfamiliar, you turn standard English into Pig Latin as follows:

* If the word begins with a consonant-sounding letter or letters, move them to the end of the word adding "ay": "happy" becomes "appy-hay", "chimp" becomes "imp-chay" and "yes" becomes "es-yay".
* In words that begin with a vowel, just append the syllable "way": "another" becomes "another-way", "elephant" becomes "elephant-way".

<<pig_latin_translator>> is a program to do that translation. It's written in Wukong, a simple library to rapidly develop big data analyses. Like the chimpanzees, it is single-concern: there's nothing in there about loading files, parallelism, network sockets or anything else. Yet you can run it over a text file from the commandline or run it over petabytes on a cluster (should you somehow have a petabyte crying out for pig-latinizing).


[[pig_latin_translator]]
.Pig Latin translator, actual version
----
    CONSONANTS   = "bcdfghjklmnpqrstvwxz"
    UPPERCASE_RE = /[A-Z]/
    PIG_LATIN_RE = %r{
      \b                  # word boundary
      ([#{CONSONANTS}]*)  # all initial consonants
      ([\w\']+)           # remaining wordlike characters
      }xi

    each_line do |line|
      latinized = line.gsub(PIG_LATIN_RE) do
        head, tail = [$1, $2]
        head       = 'w' if head.blank?
        tail.capitalize! if head =~ UPPERCASE_RE
        "#{tail}-#{head.downcase}ay"
      end
      yield latinized
    end
----

[[pig_latin_translator]]
.Pig Latin translator, pseudocode
----
    for each line,
      recognize each word in the line and change it as follows:
        separate the head consonants (if any) from the tail of the word
	if there were no initial consonants, use 'w' as the head
        give the tail the same capitalization as the word
        change the word to "{tail}-#{head}ay"
      end
      emit the latinized version of the line
    end
----

.Ruby helper
****
* The first few lines define "regular expressions" selecting the initial characters (if any) to move. Since the names are in ALL CAPS, Ruby will bind each as a constant.
* Wukong calls the `each_line do ... end` block with each line; the `|line|` part puts it in the `line` variable.
* the `gsub` ("globally substitute") statement calls its `do ... end` block with each matched word, and replaces that word with the last line of the block.
* the `yield latinized` line hands the translated string to wukong to output
****

To test the program on the commandline, run
    
    wu-local examples/text/pig_latin.rb data/magi.txt -

The last line of its output should look like

    Everywhere-way ey-thay are-way isest-way. Ey-thay are-way e-thay agi-may.

So that's what it looks like when a `cat` is feeding the program data; let's see how it works when an elephant is setting the pace.
    
==== Chimpanzee and Elephant: A Day at Work ====

Each day, the chimpanzee's foreman, a gruff silverback named J.T., hands out the day's translation manual to all the chimps.

Throughout the day, he also coordinates assigning each block of pages as each chimp signals the need for a fresh assignment. Some passages are harder than others, and you don't want some chimps to be goofing off while others are stuck with Troilus and Cressida into Kinyarwanda, so it's important that any elephant can deliver page blocks to any chimpanzee. On the other hand, sending page blocks all around would take up time and clog the hallways. 

The elephant's chief librarian, Nanette, employs several tricks to avoid this congestion.

Since each chimpanzee typically shares a cubicle with an elephant, it's much more convenient to just hand a new page block across the desk then it is to carry it down the hall. J.T. assigns tasks accordingly, using a manifest of page blocks he requests from Nanette footnote:[They work remarkably well together -- if most tasks aren't 'local', it means someone is off their game.].

Second, the page blocks of each play are not kept together -- they're distributed all around the office. So one elephant might have pages from Act I of _Hamlet_, Act II of _The Tempest_, and the first four scenes of _King Lear_. Also, there isn't just one copy of each play; there are typically three 'replicas' of each book collectively on hand footnote:[Does that sound complicated? It is -- Nanette is able to keep track of all those blocks, but if she calls in sick, nobody can get anything done. You do NOT want Nanette to call in sick.]. So even if a chimp falls behind, JT can count on one of his colleagues having a cubicle-local replica. (There's another benefit to having multiple copies: it ensures there's always a copy available. If one elephant is absent for the day, leaving her desk locked, Nanette will direct someone to make a xerox copy from either of the two other replicas.)

Nanette and J.T. exercise a bunch more savvy optimizations (like handing out the longest passages first, or having folks who finish early pitch in so everyone can go home at the same time, and more). There's no better demonstration of power through simplicity.

==== Running a Hadoop Job ====

_Note: this assumes you have a working Hadoop cluster, however large or small; if not, see <<hadoop_cluster_howto>>_

First, copy the data onto the cluster.

    wu-mkdir ./data
    wu-put   wukong_example_data/text ./data/

These commands understand `./data/text` to be a path on the HDFS, not your local disk. (An aside, to Unix users: it's OK to think of your HDFS home directory being the permanent "current" directory, so use `.` where you'd otherwise say `~`.) The `wu-put` command, which takes a list of local paths and copies them to the HDFS, treats its final argument as an HDFS path by default, and all the preceding paths as being local.

First, let's test on the same tiny little file we used at the commandline. Make sure to notice how much longer it takes this elephant to squash a flea than it did when you ran it earlier.

    wukong launch examples/text/pig_latin.rb ./data/text/magi.txt ./output/latinized_magi

It should output a bunch of happy garbage to your screen, and within a few seconds appear on the jobtracker window. The whole job should complete in far less time than it took to set it up. You can compare its output to the earlier by running

    wu-cat ./output/latinized_magi/

Now let's run it on the full Shakespeare corpus. Even this is hardly enough data to make Hadoop break a sweat, but it does show off the power of distributed computing.

    wukong launch examples/text/pig_latin.rb ./data/text/magi.txt ./output/latinized_magi

   
==== Brief Anatomy of a Hadoop Job ====

We'll go into much more detail in (TODO: ref), but here's a high-level picture of what's happening as the job runs.    

==== Chimpanzee and Elephant: Splits ====

I've danced around a minor but important detail that the workers take care of. The books are chopped up into set numbers of pages -- but the chimps translate _paragraphs_, not pages, and a page block boundary might happen mid-paragraph footnote:[in Hadoop, splits can technically happen anywhere, but the default and typically-most-efficient choice is to split at HDFS blocks as described here.].

On the chimp's part, it skips the first paragraph if it's partial, and carries on from there. Since there are many paragraphs in each page block, that's no big deal. When it gets to the end of the page block, it doesn't stop typing until is reaches the end of the current paragraph, even if it overhangs -- the extra pages show up just the same, even if they belonged to a different cubicle's elephant.

In practice, Hadoop users only need to worry about record splitting when writing a custom `InputFormat` or practicing advanced magick. You'll see lots of reference to it though -- it's a crucial subject for those inside the framework, but for regular users the story I just told is more than enough detail.

=== Exercises ===

==== Exercise 1.1: Three Stupid but Useful Scripts ====

Write the following scripts:

* *null.rb*      -- emits nothing.
* *identity.rb*  -- emits every line exactly as it was read in.

These are kinda stupid, but useful for testing -- see exercise 1.2 for example.

==== Exercise 1.2: Running time ====

It's important to build your intuition about what makes a program fast or slow. 

Let's run the *reverse.rb* and *piglatin.rb* scripts from this chapter, and the *null.rb* and *identity.rb* scripts from exercise 1.1, against the 30 Million Wikipedia Abstracts dataset.

First, though, write down an educated guess for how much longer each script will take than the `null.rb` script takes (use the table below). So, if you think the `reverse.rb` script will be 10% slower, write '10%'; if you think it will be 10% faster, write '- 10%'.

Next, run each script three times, mixing up the order. Write down 

* the total time of each run
* the average of those times
* the actual percentage difference in run time between each script and the null.rb script

        script     | est % incr | run 1 | run 2 | run 3 | avg run time | actual % incr |
        null:      |            |       |       |       |              |               |
        identity:  |            |       |       |       |              |               |
        reverse:   |            |       |       |       |              |               |
        pig_latin: |            |       |       |       |              |               |

Most people are surprised by the result.

==== Exercise 1.3: A Petabyte-scale `wc` command ====

Create a script, `wc.rb`, that emit the length of each line, the count of bytes it occupies, and the number of words it contains. 

Notes:

* The `String` methods `chomp`, `length`, `bytesize`, `split` are useful here.
* Do not include the end-of-line characters (`\n` or `\r`) in your count.
* As a reminder -- for English text the byte count and length are typically similar, but the funny characters in a string like "Iñtërnâtiônàlizætiøn" require more than one byte each. The character count says how many distinct 'letters' the string contains, regardless of how it's stored in the computer. The byte count describes how much space a string occupies, and depends on arcane details of how strings are stored. 
