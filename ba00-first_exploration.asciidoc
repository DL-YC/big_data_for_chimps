== First Exploration ==
[[first_exploration]]

Hadoop is a remarkably powerful tool for processing data, giving us at long last mastery over massive-scale distributed computing. More than likely, that's how you came to be reading this sentence.

What you might not yet know is that Hadoop's power comes from _embracing_, not conquering, the constraints of distributed computing; and in doing so, exposes a core simplicity that makes programming it exceptionally fun.

Hadoop's bargain is thus: you must agree to write all your programs according to single certain form, which we'll call the "Map / Reduce Haiku":

    Data flutters by
    Elephants make sturdy piles
    Number becomes thought

For any such program, Hadoop's diligent elephants will intelligently schedule the tasks across ones or dozens or thousands of machines; attend to logging, retry and error handling; distribute your data to the workers that process it; handle memory allocation, partitioning and network routing; and a myriad other details that would otherwise stand between you and insight.

Let's walk through an example. I'm going to leave out the code samples for the moment (you'll see them later in the book) so we can concentrate on the step-by-step transformation of the data.

=== Where is Barbecue?

The Wikipedia community has geolocated a great number of its articles. Not just defined places like Austin, TX, but one million deep: landmarks like Texas Memorial Stadium (where the Texas Longhorns football team plays) and Snow's BBQ (proclaimed "The Best Texas BBQ in the World") have computer-readable locations. What's more, since the connections among pages are robot-legible, links within topics can be read to imply a geolocation -- the movie "Dazed and Confused" (which took place in austin) and the artist Janis Joplin (who got her start in Austin) can be identifiably paired with the loose geolocation of Austin, TX.

Among all these first-and-second level connections, there's so much structure that it should be possible to identify the words that define the cultural "flavor" of a region -- and conversely, to identify the regional flavor of a topic. Here's how Hadoop makes answering a deep question like that simple.

JOEMAN:Comment Consider forming this into a more clear question. The analysis that proceeds is of course interesting, but you don't stage me with a very clear question. Seeking the 'flavor' of a region doesn't immediately strike me as a problem I have or can imagine having - unless I'm trying to create a Lonely Planet guide, for example. More context for the example can encourage interest and attention.

==== Summarize every page on Wikipedia

First, we need to summarize the words associated with each topic, by preparing the "word bag" from its wikipedia page. The wordbag <<wp_lexington_article>> text turns into the <<wp_lexington_wordbag>>

[[wp_lexington_article]]
._Wikipedia article on "Lexington, Texas"_
[quote, wikipedia, http://en.wikipedia.org/wiki/Lexington,_Texas]
______
Lexington is a town in Lee County, Texas, United States. ... Snow's BBQ, which Texas Monthly called "the best barbecue in Texas" and The New Yorker named "the best Texas BBQ in the world" is located in Lexington.
______

[[wp_lexington_wordbag]]
._Wordbag of Wikipedia article on "Lexington, Texas"_
------
Lexington,_Texas {("texas",4)("lexington",2),("best",2),("bbq",2),("barbecue",1), ...}
------

Next, annotate each wordbag with its geo coordinates (by joining with the page metadata):

[[wp_lexington_wordbag_and_coords]]
._Wordbag with coordinates_
------
Lexington,_Texas -97.01 30.41 {("texas",4)("lexington",2),("best",2),("bbq",2),("barbecue",1), ...}
------

==== Bin Locations

Now we're ready to summarize words by region: divide the world into grid cells and map each wordbag onto its article's grid cell.

------
Lexington,_Texas 023130130 {("texas",4)("lexington",2),("best",2),("bbq",2),("barbecue",1), ...}
------

The two coordinates column has turned into a funny-looking single number -- this is the "quad key", a grid scheme well-suited to big data (see chapter on <<quadkey,"Geographic Data">>). For now, just know that it's a <<unique key for each grid,quadkey_central_texas>>.

[[quadkey_central_texas]]
image::images/Quadtree-google_maps_screenshot.png[Grid tiles for Central Texas]

Next, combine the individual word bags to find each grid cell's word bag:

------
023130130 {(("many", X),...,("texas",X),...,("town",X)...("longhorns",X),...("bbq",X),...}
------

==== A pause, to think

Let's pause, take a breath, and examine the data we've produced.

We've turned articles that have a geolocation into coarse-grained regions that have implied frequencies for words. The particular frequencies arise from this combination of forces:

* _signal_: Words that describe aspects of the human condition specific to each region, like "longhorns" or "barbecue", and direct references to place names, such as "Austin" or "Texas"
* _background_: The natural frequency of each word: ("second" is used more often than "syzygy"), adjusted by the frequency each word is used in geo-locatable texts (the word "town" occurs far more frequently than its natural rate, simply because towns are geolocatable).
* _noise_: Deviations introduced by the fact that we have a limited sample of text to draw inferences from.

Our next task -- the sprint home -- is to separate the signal from the background and (as much as possible) from the noise.

Take a quick backward glance, though, to see the fundamental pattern that got us here. We

. transformed articles into wordbags
. augmented each wordbag with coordinates, using a join
. converted each article's precise point into the coarse-grained tile it sits on
. brought all wordbags for each tile together;
. merging each tile's word counts into a single combined wordbag.

It's a simple sequence of _transforms_ (operations on each record in isolation: steps 1, 3 and 5) and _reshapes_ -- operations that combine multiple rows, from different tables (the join in step 2) or in the same dataset (the group in step 4).

==== Pulling signal from noise

To expose signal from the noise, we'll pull out a trick called "Pointwise Mutual Information" (PMI) <<pmi>>. Though it may sound like an insurance holding company, in fact PMI is a simple approach that isolates the background and noise by comparing these rates:

* the rate the word 'barbecue' is used
* the rate that words are used on grid cell 023130130
* the rate the word 'barbecue' is used on grid cell 023130130

We can continue just as above to get those figures: group the data (globally, and each tile); calculate rates in each group; and combine specific rates to form the PMI scores. Rather than step through each operation, I'll wave my hands and pull its output from the oven:

------
023130130 {(("texas",X),...,("longhorns",X),...("bbq",X),...,...}
------

Finally, use a data visualization tool to see the result. As expected, you see BBQ loom large over Texas and the Southern US; Wine, over the Napa Valley.

.Not the actual output, but gives you the picture; TODO insert actual results
image::images/baldridge-bbq_wine_beach_mountain-480.jpg[Location affinity for Beach, Mountain, BBQ and Wine]

footnote:[You can use a fancier version of the approach used here to geolocate texts _based purely on their content_. An article mentioning barbecue and Willie Nelson would be placed near Austin, TX; one mentioning startups and trolleys in San Francisco. (see Baldridge et al TODO: reference)]

==== Takeaways

We accomplished a fairly sophisticated data exploration without doing anything complex. Instead of writing a big hairy monolithic program, we wrote a series of simple scripts that either _transformed_ or _reshaped_ the data. 

As you'll see, the scripts are readable and short (none exceed a few dozen lines of code). They run easily against sample data on your desktop, with no Hadoop cluster in sight; and they will then run, unchanged, against the whole of Wikipedia on dozens or hundreds of machines in a Hadoop cluster.

That's the approach we'll follow through this book: develop simple, maintainable transform/reshape scripts by iterating quickly and always keeping the data visible; then confidently transition those scripts to production as the search for a question becomes the rote production of an answer.

The challenge, then, isn't to learn to "program" Hadoop -- it's to learn how to think at scale, to choose a workable series of chess moves connecting the data you have to the insight you need. In the first part of the book, after briefly becoming familiar with the basic framework, we'll proceed through a series of examples to help you identify the key locality and thus the transformation each step calls for. In the second part of that book, we'll apply this to a range of interesting problems and so build up a set of reusable tools for asking deep questions in actual practice. 
